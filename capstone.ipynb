{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "\n",
    "# Set the path to your original dataset\n",
    "original_dataset_path = 'D:/capstone_project/skin_cancer/pre_processed_images'\n",
    "\n",
    "# Output directory for training and testing\n",
    "destination_folder = 'D:/capstone_project/skin_cancer'\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(os.path.join(destination_folder, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(destination_folder, 'test'), exist_ok=True)\n",
    "\n",
    "def copy_images(image_paths, destination_folder, class_name):\n",
    "    class_folder_train = os.path.join(destination_folder, 'train', class_name)\n",
    "    class_folder_test = os.path.join(destination_folder, 'test', class_name)\n",
    "\n",
    "    os.makedirs(class_folder_train, exist_ok=True)\n",
    "    os.makedirs(class_folder_test, exist_ok=True)\n",
    "\n",
    "    train_images, test_images = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "    for img_path in train_images:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        destination_path = os.path.join(class_folder_train, img_name)\n",
    "        try:\n",
    "            copyfile(img_path, destination_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "\n",
    "    for img_path in test_images:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        destination_path = os.path.join(class_folder_test, img_name)\n",
    "        try:\n",
    "            copyfile(img_path, destination_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "\n",
    "# Get a list of all image paths in the original dataset\n",
    "image_paths = []\n",
    "for root, dirs, files in os.walk(original_dataset_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "            image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Copy images to the training and testing directories with class-wise folders\n",
    "for class_name in os.listdir(original_dataset_path):\n",
    "    class_images = [img for img in image_paths if os.path.join(class_name) in img]\n",
    "    copy_images(class_images, destination_folder, class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6411 images belonging to 7 classes.\n",
      "Found 1599 images belonging to 7 classes.\n",
      "Epoch 1/100\n",
      "201/201 [==============================] - 409s 2s/step - loss: 1.0565 - accuracy: 0.6625 - val_loss: 0.9261 - val_accuracy: 0.6917\n",
      "Epoch 2/100\n",
      "201/201 [==============================] - 253s 1s/step - loss: 0.9157 - accuracy: 0.6770 - val_loss: 0.8947 - val_accuracy: 0.6829\n",
      "Epoch 3/100\n",
      "201/201 [==============================] - 239s 1s/step - loss: 0.8978 - accuracy: 0.6830 - val_loss: 0.8394 - val_accuracy: 0.7086\n",
      "Epoch 4/100\n",
      "201/201 [==============================] - 239s 1s/step - loss: 0.8519 - accuracy: 0.6969 - val_loss: 0.8133 - val_accuracy: 0.7067\n",
      "Epoch 5/100\n",
      "201/201 [==============================] - 250s 1s/step - loss: 0.8255 - accuracy: 0.7030 - val_loss: 0.8340 - val_accuracy: 0.6973\n",
      "Epoch 6/100\n",
      "201/201 [==============================] - 246s 1s/step - loss: 0.7701 - accuracy: 0.7178 - val_loss: 0.7974 - val_accuracy: 0.7179\n",
      "Epoch 7/100\n",
      "201/201 [==============================] - 249s 1s/step - loss: 0.7519 - accuracy: 0.7256 - val_loss: 0.7729 - val_accuracy: 0.7217\n",
      "Epoch 8/100\n",
      "201/201 [==============================] - 252s 1s/step - loss: 0.7347 - accuracy: 0.7283 - val_loss: 0.7647 - val_accuracy: 0.7236\n",
      "Epoch 9/100\n",
      "201/201 [==============================] - 239s 1s/step - loss: 0.7112 - accuracy: 0.7404 - val_loss: 0.7764 - val_accuracy: 0.7248\n",
      "Epoch 10/100\n",
      "201/201 [==============================] - 234s 1s/step - loss: 0.6848 - accuracy: 0.7443 - val_loss: 0.7345 - val_accuracy: 0.7373\n",
      "Epoch 11/100\n",
      "201/201 [==============================] - 210s 1s/step - loss: 0.6607 - accuracy: 0.7590 - val_loss: 0.6753 - val_accuracy: 0.7498\n",
      "Epoch 12/100\n",
      "201/201 [==============================] - 210s 1s/step - loss: 0.6418 - accuracy: 0.7606 - val_loss: 0.7339 - val_accuracy: 0.7480\n",
      "Epoch 13/100\n",
      "201/201 [==============================] - 220s 1s/step - loss: 0.6224 - accuracy: 0.7724 - val_loss: 0.7585 - val_accuracy: 0.7442\n",
      "Epoch 14/100\n",
      "201/201 [==============================] - 220s 1s/step - loss: 0.5959 - accuracy: 0.7835 - val_loss: 0.7810 - val_accuracy: 0.7398\n",
      "Epoch 15/100\n",
      "201/201 [==============================] - 221s 1s/step - loss: 0.5739 - accuracy: 0.7872 - val_loss: 0.7260 - val_accuracy: 0.7367\n",
      "Epoch 16/100\n",
      "201/201 [==============================] - 219s 1s/step - loss: 0.5423 - accuracy: 0.7996 - val_loss: 0.7092 - val_accuracy: 0.7398\n",
      "Epoch 17/100\n",
      "201/201 [==============================] - 231s 1s/step - loss: 0.5269 - accuracy: 0.8055 - val_loss: 0.7360 - val_accuracy: 0.7505\n",
      "Epoch 18/100\n",
      "201/201 [==============================] - 220s 1s/step - loss: 0.5106 - accuracy: 0.8136 - val_loss: 0.7143 - val_accuracy: 0.7517\n",
      "Epoch 19/100\n",
      "201/201 [==============================] - 284s 1s/step - loss: 0.4806 - accuracy: 0.8189 - val_loss: 0.7375 - val_accuracy: 0.7411\n",
      "Epoch 20/100\n",
      "201/201 [==============================] - 216s 1s/step - loss: 0.4647 - accuracy: 0.8228 - val_loss: 0.7603 - val_accuracy: 0.7336\n",
      "Epoch 21/100\n",
      "201/201 [==============================] - 211s 1s/step - loss: 0.4489 - accuracy: 0.8358 - val_loss: 0.7869 - val_accuracy: 0.7455\n",
      "Epoch 22/100\n",
      "201/201 [==============================] - 210s 1s/step - loss: 0.4268 - accuracy: 0.8406 - val_loss: 0.7780 - val_accuracy: 0.7530\n",
      "Epoch 23/100\n",
      "201/201 [==============================] - 208s 1s/step - loss: 0.3903 - accuracy: 0.8584 - val_loss: 0.9091 - val_accuracy: 0.7348\n",
      "Epoch 24/100\n",
      "201/201 [==============================] - 210s 1s/step - loss: 0.3995 - accuracy: 0.8506 - val_loss: 0.8774 - val_accuracy: 0.7486\n",
      "Epoch 25/100\n",
      "201/201 [==============================] - 209s 1s/step - loss: 0.3676 - accuracy: 0.8618 - val_loss: 0.8114 - val_accuracy: 0.7461\n",
      "Epoch 26/100\n",
      "201/201 [==============================] - 209s 1s/step - loss: 0.3531 - accuracy: 0.8727 - val_loss: 0.8859 - val_accuracy: 0.7411\n",
      "Epoch 27/100\n",
      "201/201 [==============================] - 220s 1s/step - loss: 0.3416 - accuracy: 0.8762 - val_loss: 0.9042 - val_accuracy: 0.7430\n",
      "Epoch 28/100\n",
      "201/201 [==============================] - 222s 1s/step - loss: 0.3065 - accuracy: 0.8910 - val_loss: 0.8799 - val_accuracy: 0.7386\n",
      "Epoch 29/100\n",
      "201/201 [==============================] - 221s 1s/step - loss: 0.2987 - accuracy: 0.8955 - val_loss: 1.0316 - val_accuracy: 0.7417\n",
      "Epoch 30/100\n",
      "201/201 [==============================] - 222s 1s/step - loss: 0.3053 - accuracy: 0.8914 - val_loss: 0.8086 - val_accuracy: 0.7455\n",
      "Epoch 31/100\n",
      "201/201 [==============================] - 223s 1s/step - loss: 0.2763 - accuracy: 0.9039 - val_loss: 0.9551 - val_accuracy: 0.7467\n",
      "Epoch 32/100\n",
      "201/201 [==============================] - 226s 1s/step - loss: 0.2694 - accuracy: 0.9020 - val_loss: 1.0121 - val_accuracy: 0.7486\n",
      "Epoch 33/100\n",
      "201/201 [==============================] - 286s 1s/step - loss: 0.2507 - accuracy: 0.9092 - val_loss: 1.0607 - val_accuracy: 0.7517\n",
      "Epoch 34/100\n",
      "201/201 [==============================] - 263s 1s/step - loss: 0.2408 - accuracy: 0.9144 - val_loss: 1.0988 - val_accuracy: 0.7236\n",
      "Epoch 35/100\n",
      "201/201 [==============================] - 253s 1s/step - loss: 0.2242 - accuracy: 0.9172 - val_loss: 1.1739 - val_accuracy: 0.7348\n",
      "Epoch 36/100\n",
      "201/201 [==============================] - 251s 1s/step - loss: 0.2163 - accuracy: 0.9220 - val_loss: 1.0474 - val_accuracy: 0.7217\n",
      "Epoch 37/100\n",
      "201/201 [==============================] - 252s 1s/step - loss: 0.2159 - accuracy: 0.9262 - val_loss: 1.0542 - val_accuracy: 0.7486\n",
      "Epoch 38/100\n",
      "201/201 [==============================] - 251s 1s/step - loss: 0.1947 - accuracy: 0.9323 - val_loss: 1.2135 - val_accuracy: 0.7461\n",
      "Epoch 39/100\n",
      "201/201 [==============================] - 253s 1s/step - loss: 0.1770 - accuracy: 0.9398 - val_loss: 1.1360 - val_accuracy: 0.7455\n",
      "Epoch 40/100\n",
      "201/201 [==============================] - 251s 1s/step - loss: 0.1646 - accuracy: 0.9442 - val_loss: 1.3772 - val_accuracy: 0.7361\n",
      "Epoch 41/100\n",
      "201/201 [==============================] - 251s 1s/step - loss: 0.1923 - accuracy: 0.9359 - val_loss: 1.2323 - val_accuracy: 0.7373\n",
      "Epoch 42/100\n",
      "201/201 [==============================] - 251s 1s/step - loss: 0.1824 - accuracy: 0.9356 - val_loss: 1.2070 - val_accuracy: 0.7342\n",
      "Epoch 43/100\n",
      "201/201 [==============================] - 252s 1s/step - loss: 0.1638 - accuracy: 0.9437 - val_loss: 1.1508 - val_accuracy: 0.7255\n",
      "Epoch 44/100\n",
      "201/201 [==============================] - 252s 1s/step - loss: 0.1510 - accuracy: 0.9490 - val_loss: 1.2538 - val_accuracy: 0.7348\n",
      "Epoch 45/100\n",
      "201/201 [==============================] - 250s 1s/step - loss: 0.1753 - accuracy: 0.9403 - val_loss: 1.2625 - val_accuracy: 0.7398\n",
      "Epoch 46/100\n",
      " 55/201 [=======>......................] - ETA: 2:47 - loss: 0.1039 - accuracy: 0.9682"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Set the path to your dataset\n",
    "dataset_path = 'D:/capstone_project/skin_cancer/train'\n",
    "\n",
    "# Define constants\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80% for training, 20% for validation\n",
    ")\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_height, img_width, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))  # Adjust based on the number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=100, validation_data=validation_generator)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('skin_cancer_cnn_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Segmentation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024330.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024378.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024386.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024402.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024411.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024431.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024436.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_16776\\2349447505.py:49: UserWarning: D:\\capstone_project\\skin_cancer\\pre_segmented\\ISIC_0024512.jpg is a low contrast image\n",
      "  io.imsave(output_file, segmented_image)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     55\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path, output_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path, output_path):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Read the image\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Normalize pixel values to the range [0, 1]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     normalized_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\skimage\\io\\_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[1;32m---> 53\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcall_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mplugin_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\skimage\\io\\manage_plugins.py:205\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mimageio_imread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mplugin_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\imageio\\core\\imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     plugin_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcandidate_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InitializationError:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# file extension doesn't match file type\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\imageio\\plugins\\pillow.py:91\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mio_mode \u001b[38;5;241m==\u001b[39m IOMode\u001b[38;5;241m.\u001b[39mread:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;66;03m# Check if it is generally possible to read the image.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m             \u001b[38;5;66;03m# This will not read any data and merely try to find a\u001b[39;00m\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;66;03m# compatible pillow plugin (ref: the pillow docs).\u001b[39;00m\n\u001b[0;32m     95\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnidentifiedImageError:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\imageio\\core\\request.py:492\u001b[0m, in \u001b[0;36mRequest.get_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri_type \u001b[38;5;241m==\u001b[39m URI_ZIPPED:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Get the correct filename\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     filename, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage import exposure, io, img_as_ubyte\n",
    "from skimage.filters import gaussian\n",
    "import numpy as np\n",
    "\n",
    "# Set the path to your dataset\n",
    "dataset_path = r'D:\\capstone_project\\skin_cancer\\HAM10000_images_part_1'\n",
    "\n",
    "# Output directory for preprocessed images\n",
    "output_path = r'D:\\capstone_project\\skin_cancer\\pre_segmented'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path, output_path):\n",
    "    # Read the image\n",
    "    image = io.imread(image_path)\n",
    "\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image = image.astype('float') / 255.0\n",
    "    \n",
    "    # Apply contrast enhancement using adaptive histogram equalization\n",
    "    enhanced_image = exposure.equalize_adapthist(normalized_image)\n",
    "\n",
    "    # Apply noise cancellation using Gaussian blur\n",
    "    denoised_image = gaussian(enhanced_image, sigma=0.5)\n",
    "\n",
    "    # Convert the denoised image to uint8 before saving\n",
    "    denoised_image_uint8 = img_as_ubyte(denoised_image)\n",
    "\n",
    "    # Convert image to HSV color space\n",
    "    hsv_image = cv2.cvtColor(denoised_image_uint8, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Define lower and upper bounds for non-cancerous skin color in HSV\n",
    "    lower_bound = np.array([0, 30, 10])  # Adjust these values based on your requirements\n",
    "    upper_bound = np.array([20, 150, 255])  # Adjust these values based on your requirements\n",
    "\n",
    "    # Create a binary mask using inRange function\n",
    "    mask = cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "\n",
    "    # Set non-cancerous regions to orange and cancerous to original color\n",
    "    segmented_image = image.copy()\n",
    "    segmented_image[mask == 0] = [255, 165, 0]  # Orange color for non-cancerous regions\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    output_file = os.path.join(output_path, os.path.basename(image_path))\n",
    "    io.imsave(output_file, segmented_image)\n",
    "\n",
    "# Apply preprocessing to each image in the dataset\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            preprocess_image(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> testing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at skin_cancer_cnn_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m test_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/capstone_project/skin_cancer/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the trained CNN model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskin_cancer_cnn_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define constants\u001b[39;00m\n\u001b[0;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\legacy\\save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         )\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    241\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at skin_cancer_cnn_model.h5"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Set the path to your test dataset\n",
    "test_dataset_path = 'D:/capstone_project/skin_cancer/test'\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = load_model('skin_cancer_cnn_model.h5')\n",
    "\n",
    "# Define constants\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "\n",
    "# Create an ImageDataGenerator for normalization (no data augmentation during testing)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create a data generator for testing\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "eval_result = model.evaluate(test_generator)\n",
    "\n",
    "# Print the evaluation result (accuracy and loss)\n",
    "print(f\"Test Accuracy: {eval_result[1]*100:.2f}%\")\n",
    "print(f\"Test Loss: {eval_result[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Transfer Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2005 images belonging to 7 classes.\n",
      "Found 2005 images belonging to 7 classes.\n",
      "Epoch 1/50\n",
      "63/63 [==============================] - 635s 10s/step - loss: 1.5167 - accuracy: 0.6319 - val_loss: 0.9480 - val_accuracy: 0.6813\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 664s 11s/step - loss: 1.0188 - accuracy: 0.6728 - val_loss: 0.8943 - val_accuracy: 0.6913\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 769s 12s/step - loss: 1.0396 - accuracy: 0.6648 - val_loss: 0.8705 - val_accuracy: 0.6893\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9682 - accuracy: 0.6753"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Set the path to your dataset\n",
    "train_dataset_path = 'D:/capstone_project/skin_cancer/test'\n",
    "test_dataset_path = 'D:/capstone_project/skin_cancer/test'\n",
    "\n",
    "# Constants\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Create data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create data generator for testing (no data augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load VGG16 model with pre-trained weights (excluding top layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model with VGG16 and additional layers\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7, activation='softmax'))  # Adjust based on the number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for keeping track of filenames\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=50, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# evaluation = model.evaluate(test_generator)\n",
    "# print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\certifi\\cacert.pem\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "print(certifi.where())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REQUESTS_CA_BUNDLE'] ='C:/Users/asus/AppData/Roaming/Python/Python311/site-packages/certifi/cacert.pem'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<function label at 0x000001847CADBEC0>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, Flatten, Dense\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load your dataset and split it into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a list to store individual models\u001b[39;00m\n\u001b[0;32m     17\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:394\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[0;32m    395\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    395\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:335\u001b[0m, in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingleton array \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be considered a valid collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m x\n\u001b[0;32m    337\u001b[0m         )\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<function label at 0x000001847CADBEC0>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from cProfile import label\n",
    "from pyexpat import features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load your dataset and split it into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list to store individual models\n",
    "models = []\n",
    "\n",
    "# Train multiple individual models\n",
    "for i in range(5):\n",
    "    # Create a random forest classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=i)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Add the trained model to the list\n",
    "    models.append(model)\n",
    "\n",
    "# Make predictions using each individual model\n",
    "predictions = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Perform majority voting to get the final ensemble prediction\n",
    "ensemble_predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    votes = [predictions[j][i] for j in range(len(models))]\n",
    "    majority_vote = max(set(votes), key=votes.count)\n",
    "    ensemble_predictions.append(majority_vote)\n",
    "\n",
    "# Calculate the accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy}\")\n",
    "# Set the path to your dataset\n",
    "dataset_path = 'D:/capstone_project/skin_cancer/test'\n",
    "\n",
    "# Define constants\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation and normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80% for training, 20% for validation\n",
    ")\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset\n",
    ")\n",
    "\n",
    "# Define the individual models\n",
    "models = []\n",
    "for i in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_height, img_width, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))  # Adjust based on the number of classes\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models.append(model)\n",
    "\n",
    "# Train the individual models\n",
    "for model in models:\n",
    "    model.fit(train_generator, epochs=100, validation_data=validation_generator)\n",
    "\n",
    "# Make predictions using each individual model\n",
    "predictions = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(validation_generator)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Perform majority voting to get the final ensemble prediction\n",
    "ensemble_predictions = []\n",
    "for i in range(len(validation_generator)):\n",
    "    votes = [predictions[j][i] for j in range(len(models))]\n",
    "    majority_vote = np.argmax(np.sum(votes, axis=0))\n",
    "    ensemble_predictions.append(majority_vote)\n",
    "\n",
    "# Get the ground truth labels\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Calculate the accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_true, ensemble_predictions)\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficient net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Set the path to your dataset\n",
    "dataset_path = 'D:/capstone_project/skin_cancer/pre_processed_images'\n",
    "\n",
    "\n",
    "# Constants\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80% for training, 20% for validation\n",
    ")\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset\n",
    ")\n",
    "\n",
    "# Load pre-trained EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model with EfficientNetB3 and additional layers\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7, activation='softmax'))  # Adjust based on the number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=50, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
